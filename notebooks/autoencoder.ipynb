{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dab364ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (101) must match the size of tensor b (128) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 385\u001b[0m\n\u001b[1;32m    381\u001b[0m patches_batch \u001b[38;5;241m=\u001b[39m patches_batch\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# (batch_size, num_patches, patch_feature_dim)\u001b[39;00m\n\u001b[1;32m    383\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 385\u001b[0m loss, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    388\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth2/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth2/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 355\u001b[0m, in \u001b[0;36mAudioMAERefined.forward\u001b[0;34m(self, x_patches)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_patches):\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;66;03m# x_patches: (batch_size, num_patches, patch_feature_dim)\u001b[39;00m\n\u001b[1;32m    353\u001b[0m     unmasked_indices, masked_indices, ids_restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_random_mask(x_patches)\n\u001b[0;32m--> 355\u001b[0m     encoded_unmasked_patches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_patches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munmasked_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m     pred_masked_patches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_decoder(encoded_unmasked_patches, unmasked_indices, masked_indices, ids_restore)\n\u001b[1;32m    357\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_loss(x_patches, pred_masked_patches, masked_indices)\n",
      "Cell \u001b[0;32mIn[1], line 250\u001b[0m, in \u001b[0;36mAudioMAERefined.forward_encoder\u001b[0;34m(self, x_patches, unmasked_indices)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_encoder\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_patches, unmasked_indices):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# x_patches: (batch_size, num_patches, patch_feature_dim)\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed(x_patches) \u001b[38;5;66;03m# (batch_size, num_patches, encoder_dim)\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_embed\u001b[49m \u001b[38;5;66;03m# Add positional encoding\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# Gather unmasked patches\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     x_unmasked \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mgather(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, index\u001b[38;5;241m=\u001b[39munmasked_indices\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_dim))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (101) must match the size of tensor b (128) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Model Definition (Placeholder) ---\n",
    "class AudioMAE(nn.Module):\n",
    "    def __init__(self, input_dim, encoder_dim, decoder_dim, num_patches, mask_ratio=0.75):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.num_patches = num_patches\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "        # Patch embedding (e.g., a Conv1D or Linear layer)\n",
    "        # For simplicity, let's assume patches are already formed and flattened\n",
    "        self.patch_embed = nn.Linear(input_dim // num_patches, encoder_dim) # Simplified\n",
    "\n",
    "        # Encoder (e.g., a Transformer Encoder)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=encoder_dim, nhead=4, dim_feedforward=encoder_dim*2, batch_first=True),\n",
    "            num_layers=6\n",
    "        )\n",
    "\n",
    "        # Decoder (e.g., a Transformer Decoder or MLP)\n",
    "        self.decoder_embed = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_dim))\n",
    "        self.decoder = nn.TransformerEncoder( # Using Encoder for simplicity, a Decoder would be more standard\n",
    "            nn.TransformerEncoderLayer(d_model=decoder_dim, nhead=4, dim_feedforward=decoder_dim*2, batch_first=True),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.decoder_pred = nn.Linear(decoder_dim, input_dim // num_patches) # Predicts patch features\n",
    "\n",
    "    def forward_encoder(self, x, unmasked_indices):\n",
    "        # x: (batch_size, num_patches, patch_dim)\n",
    "        x = self.patch_embed(x) # (batch_size, num_patches, encoder_dim)\n",
    "\n",
    "        # Apply positional encoding (not shown for brevity, but important)\n",
    "        # x = x + self.pos_embed\n",
    "\n",
    "        # Select only unmasked patches for encoder\n",
    "        x_unmasked = torch.gather(x, dim=1, index=unmasked_indices.unsqueeze(-1).expand(-1, -1, x.shape[-1]))\n",
    "        encoded_patches = self.encoder(x_unmasked)\n",
    "        return encoded_patches\n",
    "\n",
    "    def forward_decoder(self, encoded_patches, unmasked_indices, masked_indices):\n",
    "        # encoded_patches: (batch_size, num_unmasked_patches, encoder_dim)\n",
    "        batch_size = encoded_patches.shape[0]\n",
    "        num_unmasked = unmasked_indices.shape[1]\n",
    "        num_masked = masked_indices.shape[1]\n",
    "\n",
    "        encoded_patches = self.decoder_embed(encoded_patches)\n",
    "\n",
    "        # Create full sequence with mask tokens\n",
    "        full_sequence = torch.cat([encoded_patches, self.mask_token.expand(batch_size, num_masked, -1)], dim=1)\n",
    "        # Reconstruct original order (simplified, actual MAE shuffles and unshuffles)\n",
    "        # For simplicity, we assume unmasked_indices and masked_indices together form the full sequence in order\n",
    "        # A more robust implementation would involve an unshuffle operation based on original indices.\n",
    "        # Here, we'll just decode the concatenated sequence.\n",
    "        # A proper MAE would re-introduce positional embeddings for the decoder.\n",
    "\n",
    "        decoded_patches = self.decoder(full_sequence)\n",
    "        return self.decoder_pred(decoded_patches[:, num_unmasked:]) # Predict only masked patches\n",
    "\n",
    "    def forward_loss(self, x_patches, pred_patches, masked_indices):\n",
    "        # x_patches: (batch_size, num_patches, patch_dim) - original patches\n",
    "        # pred_patches: (batch_size, num_masked_patches, patch_dim) - predicted masked patches\n",
    "        # masked_indices: (batch_size, num_masked_patches)\n",
    "\n",
    "        # Gather the ground truth masked patches\n",
    "        target_patches = torch.gather(x_patches, dim=1, index=masked_indices.unsqueeze(-1).expand(-1, -1, x_patches.shape[-1]))\n",
    "\n",
    "        loss = nn.functional.mse_loss(pred_patches, target_patches)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x_patches):\n",
    "        # x_patches: (batch_size, num_patches, patch_dim)\n",
    "        batch_size, num_patches, _ = x_patches.shape\n",
    "        num_masked = int(self.mask_ratio * num_patches)\n",
    "\n",
    "        # Generate random mask\n",
    "        # This is a simplified masking. Real MAE shuffles patches.\n",
    "        noise = torch.rand(batch_size, num_patches, device=x_patches.device)\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        unmasked_indices = ids_shuffle[:, :num_patches - num_masked]\n",
    "        masked_indices = ids_shuffle[:, num_patches - num_masked:]\n",
    "\n",
    "\n",
    "        encoded_patches = self.forward_encoder(x_patches, unmasked_indices)\n",
    "        pred_masked_patches = self.forward_decoder(encoded_patches, unmasked_indices, masked_indices)\n",
    "        loss = self.forward_loss(x_patches, pred_masked_patches, masked_indices)\n",
    "        return loss, pred_masked_patches, masked_indices, unmasked_indices\n",
    "\n",
    "\n",
    "# --- 2. Dataset and DataLoader (Placeholder) ---\n",
    "class DummyAudioDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, sample_rate=16000, duration=1, num_patches=64, feature_dim=256):\n",
    "        self.num_samples = num_samples\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration = duration\n",
    "        self.num_frames = self.sample_rate * self.duration\n",
    "        self.num_patches = num_patches # Number of patches\n",
    "        self.patch_len = self.num_frames // self.num_patches # Length of each patch in frames\n",
    "        self.feature_dim = feature_dim # For mel spectrogram\n",
    "\n",
    "        # Mel Spectrogram configuration\n",
    "        self.mel_spectrogram = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=400,\n",
    "            hop_length=160, # results in 100 frames per second for 16kHz\n",
    "            n_mels=feature_dim\n",
    "        )\n",
    "        # Ensure total frames from mel spec are divisible by num_patches\n",
    "        # For 1s audio at 16kHz, hop_length 160 -> 100 frames. If num_patches=64, this won't divide.\n",
    "        # Let's adjust num_frames for mel spectrogram output\n",
    "        # Effective frames for mel: (self.num_frames - n_fft) // hop_length + 1\n",
    "        # For simplicity, we'll generate raw audio and then process it.\n",
    "        # The actual patching strategy needs to be carefully designed.\n",
    "\n",
    "        # For this dummy dataset, we'll generate random data and \"pretend\" they are patches\n",
    "        # A real dataset would load audio, compute spectrograms, and then patch them.\n",
    "        self.patch_feature_dim = feature_dim # Each patch is a segment of the mel spectrogram\n",
    "                                            # If we flatten time within a patch, it's patch_len_mel * feature_dim\n",
    "                                            # Here, we assume patches are (num_patches, feature_dim_per_patch)\n",
    "                                            # Let's assume each \"patch\" is a time step of the mel spectrogram\n",
    "                                            # So, input_dim for AudioMAE will be feature_dim (n_mels)\n",
    "                                            # And num_patches will be the number of time steps in the mel spectrogram\n",
    "\n",
    "        # Calculate number of time frames from mel spectrogram\n",
    "        # For 1s audio, 16000 frames, n_fft=400, hop_length=160 -> (16000-400)//160 + 1 = 97.5 -> 98 frames\n",
    "        # Let's use a fixed number of frames for simplicity in this dummy example\n",
    "        self.num_mel_frames = 128 # This will be our num_patches\n",
    "        self.input_dim_mae = feature_dim # n_mels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate dummy audio waveform\n",
    "        waveform = torch.randn(self.num_frames)\n",
    "\n",
    "        # Compute Mel Spectrogram\n",
    "        mel_spec = self.mel_spectrogram(waveform) # (n_mels, num_time_frames)\n",
    "        mel_spec = mel_spec.squeeze(0) # Remove channel dim if present\n",
    "        mel_spec = mel_spec[:, :self.num_mel_frames] # Trim or pad to ensure fixed num_mel_frames\n",
    "\n",
    "        # Normalize (example)\n",
    "        mel_spec = (mel_spec - mel_spec.mean()) / (mel_spec.std() + 1e-6)\n",
    "\n",
    "        # Patches are time steps of the mel spectrogram\n",
    "        # (n_mels, num_mel_frames) -> (num_mel_frames, n_mels) to match (num_patches, patch_dim)\n",
    "        patches = mel_spec.transpose(0, 1) # (num_mel_frames, n_mels)\n",
    "        return patches\n",
    "\n",
    "\n",
    "# --- 3. Training Configuration ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 50\n",
    "num_patches_dataset = 128 # This should match DummyAudioDataset.num_mel_frames\n",
    "feature_dim_dataset = 64  # This should match DummyAudioDataset.feature_dim (n_mels)\n",
    "\n",
    "# Instantiate Dataset and DataLoader\n",
    "# Note: The input_dim for AudioMAE should be feature_dim_dataset\n",
    "# The num_patches for AudioMAE should be num_patches_dataset\n",
    "dummy_dataset = DummyAudioDataset(num_samples=200, num_patches=num_patches_dataset, feature_dim=feature_dim_dataset)\n",
    "train_loader = DataLoader(dummy_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Instantiate Model\n",
    "# The input_dim to the patch_embed in AudioMAE should be feature_dim_dataset\n",
    "# The num_patches in AudioMAE should be num_patches_dataset\n",
    "model = AudioMAE(\n",
    "    input_dim=feature_dim_dataset * num_patches_dataset, # This is if we flatten all patches\n",
    "                                                        # If patch_embed takes individual patch features:\n",
    "                                                        # input_dim should be feature_dim_dataset\n",
    "                                                        # And the model's patch_embed input should be feature_dim_dataset\n",
    "    encoder_dim=256,\n",
    "    decoder_dim=128,\n",
    "    num_patches=num_patches_dataset, # This is the sequence length for the transformer\n",
    "    mask_ratio=0.75\n",
    ").to(device)\n",
    "\n",
    "# Adjusting model init based on how DummyAudioDataset provides patches:\n",
    "# DummyAudioDataset provides (num_mel_frames, n_mels) which is (num_patches, feature_dim_per_patch)\n",
    "# So, AudioMAE's patch_embed should take feature_dim_per_patch as input.\n",
    "# The input_dim in AudioMAE's __init__ was for the *total* flattened input, let's adjust.\n",
    "# The patch_embed in AudioMAE is nn.Linear(input_dim // num_patches, encoder_dim)\n",
    "# So, input_dim // num_patches should be feature_dim_dataset.\n",
    "# Thus, the first argument to AudioMAE (input_dim) should be feature_dim_dataset * num_patches_dataset.\n",
    "# This seems correct if the model internally flattens the (num_patches, patch_feature_dim) input before patch_embed.\n",
    "# Let's refine the AudioMAE's patch_embed to directly take patch_feature_dim.\n",
    "\n",
    "class AudioMAERefined(nn.Module):\n",
    "    def __init__(self, patch_feature_dim, encoder_dim, decoder_dim, num_patches, mask_ratio=0.75):\n",
    "        super().__init__()\n",
    "        self.patch_feature_dim = patch_feature_dim # e.g., n_mels\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.num_patches = num_patches # e.g., number of time frames in mel spectrogram\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "        self.patch_embed = nn.Linear(patch_feature_dim, encoder_dim)\n",
    "\n",
    "        # Positional Encoding (learnable or fixed)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, encoder_dim))\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=encoder_dim, nhead=4, dim_feedforward=encoder_dim*2, batch_first=True, dropout=0.1),\n",
    "            num_layers=6\n",
    "        )\n",
    "\n",
    "        self.decoder_embed = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_dim))\n",
    "        # Positional Encoding for decoder (can be shared or separate)\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches, decoder_dim))\n",
    "\n",
    "\n",
    "        self.decoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=decoder_dim, nhead=4, dim_feedforward=decoder_dim*2, batch_first=True, dropout=0.1),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.decoder_pred = nn.Linear(decoder_dim, patch_feature_dim) # Predicts original patch features\n",
    "\n",
    "    def _generate_random_mask(self, x):\n",
    "        batch_size, num_patches, _ = x.shape\n",
    "        num_unmasked = int(num_patches * (1 - self.mask_ratio))\n",
    "\n",
    "        noise = torch.rand(batch_size, num_patches, device=x.device)\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        unmasked_indices = ids_shuffle[:, :num_unmasked]\n",
    "        masked_indices = ids_shuffle[:, num_unmasked:]\n",
    "        return unmasked_indices, masked_indices, ids_restore\n",
    "\n",
    "    def forward_encoder(self, x_patches, unmasked_indices):\n",
    "        # x_patches: (batch_size, num_patches, patch_feature_dim)\n",
    "        x = self.patch_embed(x_patches) # (batch_size, num_patches, encoder_dim)\n",
    "        x = x + self.pos_embed # Add positional encoding\n",
    "\n",
    "        # Gather unmasked patches\n",
    "        x_unmasked = torch.gather(x, dim=1, index=unmasked_indices.unsqueeze(-1).expand(-1, -1, self.encoder_dim))\n",
    "        encoded_patches = self.encoder(x_unmasked) # (batch_size, num_unmasked_patches, encoder_dim)\n",
    "        return encoded_patches\n",
    "\n",
    "    def forward_decoder(self, encoded_unmasked_patches, unmasked_indices, masked_indices, ids_restore):\n",
    "        # encoded_unmasked_patches: (batch_size, num_unmasked_patches, encoder_dim)\n",
    "        batch_size = encoded_unmasked_patches.shape[0]\n",
    "        num_total_patches = self.num_patches\n",
    "        num_masked_patches = masked_indices.shape[1]\n",
    "\n",
    "        # Embed encoded patches to decoder dimension\n",
    "        decoder_embedded_patches = self.decoder_embed(encoded_unmasked_patches) # (B, num_unmasked, decoder_dim)\n",
    "\n",
    "        # Create mask tokens for masked patches\n",
    "        mask_tokens = self.mask_token.expand(batch_size, num_masked_patches, -1) # (B, num_masked, decoder_dim)\n",
    "\n",
    "        # Concatenate unmasked patch embeddings and mask tokens\n",
    "        # We need to place them in the original shuffled order before unmasking\n",
    "        # This is a simplification: MAE typically appends mask tokens and then unshuffles.\n",
    "        # For a more accurate MAE:\n",
    "        # 1. Create a full sequence placeholder for the decoder.\n",
    "        # 2. Place decoder_embedded_patches into this placeholder at unmasked_indices (after shuffling).\n",
    "        # 3. Place mask_tokens into this placeholder at masked_indices (after shuffling).\n",
    "        # 4. Add decoder_pos_embed to this full sequence.\n",
    "        # 5. Unshuffle the sequence using ids_restore.\n",
    "        # 6. Pass to decoder.\n",
    "\n",
    "        # Simplified approach (less faithful to original MAE but easier to start):\n",
    "        # Assume unmasked_indices and masked_indices are sorted for concatenation\n",
    "        # This is not what MAE does. MAE restores order.\n",
    "\n",
    "        # Let's try to be more faithful:\n",
    "        # Create full sequence for decoder input\n",
    "        decoder_input_full = torch.zeros(batch_size, num_total_patches, self.decoder_dim, device=encoded_unmasked_patches.device)\n",
    "\n",
    "        # Scatter unmasked patches\n",
    "        # ids_shuffle gives the shuffled indices. unmasked_indices are the first N of these.\n",
    "        # We need to place decoder_embedded_patches at their *original* positions in the shuffled sequence\n",
    "        # This part is tricky without the exact MAE unshuffle logic.\n",
    "        # For now, let's use a simplified placeholder for decoder input construction.\n",
    "\n",
    "        # A common MAE approach:\n",
    "        # x_full for decoder: (batch_size, num_patches, decoder_dim)\n",
    "        # Initialize with mask tokens, then fill in the encoded unmasked patches\n",
    "        x_full_decoder = self.mask_token.expand(batch_size, num_total_patches, -1).clone()\n",
    "        # Scatter the embedded unmasked patches to their positions in the *shuffled* sequence\n",
    "        # unmasked_indices are like [idx_val1, idx_val2, ...] where idx_val is the original patch index\n",
    "        # We need to map these original patch indices to their positions in the *shuffled* sequence.\n",
    "        # This is what ids_shuffle and ids_restore are for.\n",
    "\n",
    "        # Let `h_encoder` be `decoder_embedded_patches`\n",
    "        # Let `ids_unmasked` be `unmasked_indices` (these are indices of original patches that are kept)\n",
    "        # Let `ids_masked` be `masked_indices` (indices of original patches that are masked)\n",
    "\n",
    "        # Create the full sequence for the decoder\n",
    "        # The length is num_total_patches.\n",
    "        # The first `num_unmasked` positions (in the shuffled sense) get `decoder_embedded_patches`.\n",
    "        # The remaining `num_masked` positions (in the shuffled sense) get `mask_tokens`.\n",
    "        # Then, add decoder positional embeddings and unshuffle.\n",
    "\n",
    "        # Step 1: Concatenate visible tokens and mask tokens\n",
    "        # decoder_embedded_patches corresponds to the first `num_unmasked` elements of the shuffled sequence.\n",
    "        # mask_tokens corresponds to the last `num_masked` elements of the shuffled sequence.\n",
    "        x_shuffled_for_decoder = torch.cat([decoder_embedded_patches, mask_tokens], dim=1)\n",
    "\n",
    "        # Step 2: Unshuffle the sequence\n",
    "        # ids_restore will map from the shuffled order back to the original patch order.\n",
    "        x_unshuffled_for_decoder = torch.gather(x_shuffled_for_decoder, dim=1,\n",
    "                                             index=ids_restore.unsqueeze(-1).expand(-1, -1, self.decoder_dim))\n",
    "\n",
    "        # Step 3: Add decoder positional embedding\n",
    "        x_unshuffled_for_decoder = x_unshuffled_for_decoder + self.decoder_pos_embed\n",
    "\n",
    "        # Step 4: Pass through decoder\n",
    "        decoded_full_sequence = self.decoder(x_unshuffled_for_decoder) # (B, num_total_patches, decoder_dim)\n",
    "\n",
    "        # Step 5: Predict only the masked patches\n",
    "        # We need to gather the outputs corresponding to the original masked_indices\n",
    "        # decoded_full_sequence is in original patch order.\n",
    "        # masked_indices contains the *original* indices of the patches that were masked.\n",
    "        pred_masked_patches = torch.gather(decoded_full_sequence, dim=1,\n",
    "                                           index=masked_indices.unsqueeze(-1).expand(-1, -1, self.decoder_dim))\n",
    "        pred_masked_patches = self.decoder_pred(pred_masked_patches) # (B, num_masked_patches, patch_feature_dim)\n",
    "        return pred_masked_patches\n",
    "\n",
    "\n",
    "    def forward_loss(self, original_patches, pred_masked_patches, masked_indices):\n",
    "        # original_patches: (batch_size, num_total_patches, patch_feature_dim)\n",
    "        # pred_masked_patches: (batch_size, num_masked_patches, patch_feature_dim)\n",
    "        # masked_indices: (batch_size, num_masked_patches) - original indices of masked patches\n",
    "\n",
    "        # Gather the ground truth for the masked patches from the original input\n",
    "        target_masked_patches = torch.gather(original_patches, dim=1,\n",
    "                                          index=masked_indices.unsqueeze(-1).expand(-1, -1, self.patch_feature_dim))\n",
    "\n",
    "        loss = nn.functional.mse_loss(pred_masked_patches, target_masked_patches)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x_patches):\n",
    "        # x_patches: (batch_size, num_patches, patch_feature_dim)\n",
    "        unmasked_indices, masked_indices, ids_restore = self._generate_random_mask(x_patches)\n",
    "\n",
    "        encoded_unmasked_patches = self.forward_encoder(x_patches, unmasked_indices)\n",
    "        pred_masked_patches = self.forward_decoder(encoded_unmasked_patches, unmasked_indices, masked_indices, ids_restore)\n",
    "        loss = self.forward_loss(x_patches, pred_masked_patches, masked_indices)\n",
    "\n",
    "        return loss, pred_masked_patches, masked_indices # ids_restore could also be returned for viz\n",
    "\n",
    "\n",
    "model = AudioMAERefined(\n",
    "    patch_feature_dim=feature_dim_dataset, # n_mels\n",
    "    encoder_dim=256, # Latent dimension of encoder\n",
    "    decoder_dim=128, # Latent dimension of decoder\n",
    "    num_patches=num_patches_dataset, # Number of time frames / patches\n",
    "    mask_ratio=0.75\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.05)\n",
    "# Loss is calculated inside the model's forward pass for MAE\n",
    "\n",
    "# --- 4. Training Loop ---\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, patches_batch in enumerate(train_loader):\n",
    "        patches_batch = patches_batch.to(device) # (batch_size, num_patches, patch_feature_dim)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, _, _ = model(patches_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 2 == 0: # Print every few batches\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# --- 5. Evaluation / Inference (Placeholder) ---\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     # Perform inference, e.g., reconstruct masked audio or extract features\n",
    "#     for patches_batch in train_loader: # Using train_loader for example\n",
    "#         patches_batch = patches_batch.to(device)\n",
    "#         loss, reconstructed_patches, masked_indices = model(patches_batch)\n",
    "#         print(f\"Evaluation Loss: {loss.item():.4f}\")\n",
    "#         # Here you could visualize the original vs reconstructed masked patches\n",
    "#         break # Just one batch for example\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
